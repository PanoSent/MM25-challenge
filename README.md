# (MERG) Avatar-based Multimodal Empathetic Conversation Challenge

The Avatar-based Multimodal Empathetic Conversation (MERG) challenge falls within the research domain of multimodal affective computing and em- pathetic dialogue generation. Unlike traditional text-only empa- thetic response tasks, the MERG challenge requires models to gen- erate contextually appropriate, emotionally aligned responses in a multimodal setting. Given a multimodal dialogue context, mod- els are expected to generate an empathetic response that includes three synchronized components: textual reply, emotive speech au- dio, and expressive talking face video. To support this challenge, a large-scale MERG dataset will be provided, consisting of high- quality multimodal dialogue-response pairs. The dataset includes diverse conversational scenarios where human annotators have carefully crafted emotionally rich responses, ensuring alignment across text, speech, and visual expressions. The challenge is struc- tured into three subtasks: (1) Text-Only Empathetic Response Gen- eration, which focuses on generating an empathetic textual reply based solely on the dialogue history; (2) Multimodal-Aware Em- pathetic Response Generation, which requires generating a tex- tual response while leveraging multimodal contextual information from speech and visual cues; and (3) Multimodal Empathetic Re- sponse Generation, which involves generating a fully multimodal response, including synchronized text, speech, and facial expres- sions, to achieve a natural and emotionally aligned interaction.
                        
Please refer to the challenge webpage: [AvaMERG challenge](https://avamerg.github.io/MM25-challenge)